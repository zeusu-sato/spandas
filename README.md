## ğŸ‡¯ğŸ‡µ Spandas - Databricks å‘ã‘ã®è»½é‡æ‹¡å¼µ

**Spandas** ã¯ã€PySpark ã® pandas API (`from pyspark import pandas as ps`) ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€pandas ã®ã‚ˆã†ãªä½¿ã„ã‚„ã™ã•ã§ Spark ä¸Šã® DataFrame æ“ä½œã‚’å¼·åŒ–ã™ã‚‹ã€Databricks ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å‘ã‘ã®è»½é‡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

### ç‰¹å¾´

- pandasã®ã‚ˆã†ãª `.apply()`, `.agg()`, `.groupby()` ãªã©ã®æ“ä½œã‚’Sparkä¸Šã§å†ç¾
- `.plot()`, `.hist()`, `.boxplot()` ã«ã‚ˆã‚‹å¯è¦–åŒ–ï¼ˆpandasçµŒç”±ï¼‰
- `to_pandas=False` ã«ã‚ˆã‚‹Sparkãƒã‚¤ãƒ†ã‚£ãƒ–ãªãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆå‡¦ç†
- `.loc`, `.iloc`, `.T`, `.pivot`, `.melt` ãªã©ã€ä½¿ã„æ…£ã‚ŒãŸAPIã‚’ã‚µãƒãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

Databricks ã§ã¯æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã ã‘ã§åˆ©ç”¨ã§ãã¾ã™:

```bash
pip install spandas
```

ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½:

- **Dask é€£æº:** `pip install "spandas[dask_legacy]"`
- **ãƒ­ãƒ¼ã‚«ãƒ« Spark æ¤œè¨¼:** `pip install "spandas[spark]"`

> **æ³¨æ„:** æœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ PySpark 3.5 ç³»ãŠã‚ˆã³ pandas 1.x ç³»ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

### ä½¿ç”¨ä¾‹

```python
from spandas import Spandas
from pyspark import pandas as ps

psdf = ps.read_csv("sample.csv")
sdf = Spandas(psdf)

sdf = sdf.dropna()
sdf["new_col"] = sdf["val"].apply(lambda x: x**2)
sdf.plot()
```

### æ§‹æˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

- `original/` ... å…ƒã®pandas-on-Sparkäº’æ›é–¢æ•°ã®é€€é¿
- `enhanced/` ... æ©Ÿèƒ½ã”ã¨ã®å¼·åŒ–é–¢æ•°ç¾¤ï¼ˆapply, selection, mathstats ãªã©ï¼‰
- `spandas.py` ... Spandasã‚¯ãƒ©ã‚¹æœ¬ä½“ï¼ˆæ‹¡å¼µæ©Ÿèƒ½ã‚’çµ±åˆï¼‰

---

## ğŸ‡ºğŸ‡¸ Spandas - Lightweight Extensions for Databricks

**Spandas** extends PySpark's pandas API (`from pyspark import pandas as ps`) to provide a pandas-like experience on Spark.

### Features

- Familiar pandas-style API on Spark: `.apply()`, `.agg()`, `.groupby()`, etc.
- Plotting via `.plot()`, `.hist()`, `.boxplot()` (backed by pandas/matplotlib)
- Best-effort native Spark execution with `to_pandas=False`
- Support for `.loc`, `.iloc`, `.T`, `.pivot`, `.melt`, and more

### Installation

On Databricks just run:

```bash
pip install spandas
```

Optional extras:

- **Dask integration:** `pip install "spandas[dask_legacy]"`
- **Local Spark testing:** `pip install "spandas[spark]"`

> **Note:** The package targets PySpark 3.5.x and pandas 1.x (Databricks Runtime compatible).

### Example

```python
from spandas import Spandas
from pyspark import pandas as ps

psdf = ps.read_csv("sample.csv")
sdf = Spandas(psdf)

sdf = sdf.dropna()
sdf["new_col"] = sdf["val"].apply(lambda x: x**2)
sdf.plot()
```

### Project Structure

- `original/` - Backups of original pandas-on-Spark methods
- `enhanced/` - Feature-specific enhancements (apply, selection, mathstats, etc.)
- `spandas.py` - Main class that binds all enhanced functionality

---

## ğŸ“„ License / ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

- English: This project is licensed under the **MIT License** â€“ see the [LICENSE](./LICENSE) file for details.
- æ—¥æœ¬èª: æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ **MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹** ã®ã‚‚ã¨ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯ [LICENSE](./LICENSE) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
